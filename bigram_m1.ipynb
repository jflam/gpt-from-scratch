{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Karpathy Bigram Language Model\n",
    "\n",
    "This is my Jupyter notebook where I replicate what Andrej teaches me in his\n",
    "YouTube video: https://www.youtube.com/watch?v=kCc8FmEb1nY. The notebook \n",
    "contains the part of the video (first ~40 minutes) where he walks us through\n",
    "how to train a simple Bigram Language model using the `tinyshakespeare` \n",
    "dataset to make predictions using only the last character in the sequence.\n",
    "\n",
    "This is a good pedagogical introduction to the general problem of training\n",
    "models, of pulling random chunks of text out of the training corpus to \n",
    "train on. How to use \n",
    "\n",
    "## GPU optimization using Metal\n",
    "Learning about batch sizes. The inference function, `generate()` is not \n",
    "optimized to use batching, so it is extremely slow on GPU vs. CPU. This is \n",
    "a bit strange because of the entire idea of unified memory on Macs. This\n",
    "is something to delve into more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "First cell in the notebook downloads the `tinyshakespeare` dataset into this\n",
    "repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-12 14:02:12--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.3’\n",
      "\n",
      "input.txt.3         100%[===================>]   1.06M  4.32MB/s    in 0.2s    \n",
      "\n",
      "2023-03-12 14:02:12 (4.32 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print('corpus length:', len(text))\n",
    "print(text[:300])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the vocabulary to use for the characters in the dataset since we're\n",
    "building a character-based (NOT token-based) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars=sorted(list(set(text)))\n",
    "vocab_size=len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some dicts that will be used for forward/reverse mapping of chars to tokens (integers in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "print(stoi)\n",
    "print(itos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write some functions that will encode and decode to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[c] for c in l])\n",
    "\n",
    "print(encode('hii there'))\n",
    "print(decode(encode('hii there')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same thing but using a real tokenizer, [tiktoken](https://github.com/openai/tiktoken). There are multiple encoders that can be used:\n",
    "\n",
    "- `cl100k_base`: encoder used by ChatGPT models\n",
    "- `gpt2`: encoder used by older models like GPT3 which has a smaller vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100277\n",
      "[71, 3893, 1070]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('cl100k_base')\n",
    "print(enc.n_vocab)\n",
    "print(enc.encode(\"hii there\"))\n",
    "print(enc.decode(enc.encode(\"hii there\")))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trade-off here is that there are token sequences using a character tokenizer, but things are simpler in the end."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize pytorch for this notebook\n",
    "\n",
    "Using the `mps` device on Apple Silicon to accelerate pytorch computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Metal backend\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F \n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"Using Metal backend\")\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# FORCE CPU\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the entire Shakespeare corpus into a pytorch tensor. You can see that it is a 1 dimensional tensor with the same number of elements as characters from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long).to(device)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:300])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training/validation split. 90% will be train, 10% will be validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12,  0,  0,  ..., 45,  8,  0])\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(val_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input is never fed in its entirety into the transformer. Instead it is fed in chunks, with the `block_size` variable controlling the size of the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the transformer is to predict the next character in the sequence within the block, but also the next character given a context. The context is key to the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to think about here is the batch size. We will feed `block_size` chunks into the transformer in batches for efficiency. The number of batches is controlled by `batch_size`. The `get_batch` function below will compute a vector of random integers for offsets (`ix`) and use the `torch.stack()` function to stack the vectors into a two dimensional array. \n",
    "\n",
    "Therefore, each block is a vector of length `block_size` and we turn that into a two dimensional tensor that \"stacks\" each vector as rows. So the number of rows in each batch is of `batch_size`. Also note that the targets are just each input shifted to the right by one character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
      "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
      "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
      "        [ 1, 39,  1, 46, 53, 59, 57, 43]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[59,  6,  1, 58, 56, 47, 40, 59],\n",
      "        [43, 43, 54,  1, 47, 58,  1, 58],\n",
      "        [52, 45, 43, 50, 53,  8,  0, 26],\n",
      "        [39,  1, 46, 53, 59, 57, 43,  0]])\n",
      "----\n",
      "when input is [53] the target: 59\n",
      "when input is [53, 59] the target: 6\n",
      "when input is [53, 59, 6] the target: 1\n",
      "when input is [53, 59, 6, 1] the target: 58\n",
      "when input is [53, 59, 6, 1, 58] the target: 56\n",
      "when input is [53, 59, 6, 1, 58, 56] the target: 47\n",
      "when input is [53, 59, 6, 1, 58, 56, 47] the target: 40\n",
      "when input is [53, 59, 6, 1, 58, 56, 47, 40] the target: 59\n",
      "when input is [49] the target: 43\n",
      "when input is [49, 43] the target: 43\n",
      "when input is [49, 43, 43] the target: 54\n",
      "when input is [49, 43, 43, 54] the target: 1\n",
      "when input is [49, 43, 43, 54, 1] the target: 47\n",
      "when input is [49, 43, 43, 54, 1, 47] the target: 58\n",
      "when input is [49, 43, 43, 54, 1, 47, 58] the target: 1\n",
      "when input is [49, 43, 43, 54, 1, 47, 58, 1] the target: 58\n",
      "when input is [13] the target: 52\n",
      "when input is [13, 52] the target: 45\n",
      "when input is [13, 52, 45] the target: 43\n",
      "when input is [13, 52, 45, 43] the target: 50\n",
      "when input is [13, 52, 45, 43, 50] the target: 53\n",
      "when input is [13, 52, 45, 43, 50, 53] the target: 8\n",
      "when input is [13, 52, 45, 43, 50, 53, 8] the target: 0\n",
      "when input is [13, 52, 45, 43, 50, 53, 8, 0] the target: 26\n",
      "when input is [1] the target: 39\n",
      "when input is [1, 39] the target: 1\n",
      "when input is [1, 39, 1] the target: 46\n",
      "when input is [1, 39, 1, 46] the target: 53\n",
      "when input is [1, 39, 1, 46, 53] the target: 59\n",
      "when input is [1, 39, 1, 46, 53, 59] the target: 57\n",
      "when input is [1, 39, 1, 46, 53, 59, 57] the target: 43\n",
      "when input is [1, 39, 1, 46, 53, 59, 57, 43] the target: 0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b,:t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Bigram language model (see his other video for details for how it works). Note that we can see the shape of the module as (`batch_size`, `block_size`, `vocab_size`). For each token, it retrieves a row from the Embedding table which are the logits which is the probability of each token. So given a token of 4 in batch 0, we will have a vector of length `vocab_size` that represents the probability of each token predicted by the model. So we can do things like retrieve the highest probability or some other algorithm to generate the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        logits = self.token_embedding_table(idx).to(device)\n",
    "        return logits \n",
    "    \n",
    "m = BigramLanguageModel(vocab_size).to(device)\n",
    "out = m(xb, yb).to(device)\n",
    "print(out.shape)\n",
    "print(out.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have here is a (B,T,C) tensor, but the `F.cross_entropy()` function expects a (B,C,T) tensor as input. We will accomplish this by collapsing the B and T dimensions into a single dimension - what Andrej refers to in his video as \"stretching out\" the tensor in those directions. We will do it for both the inputs and the targets via the view() method of the tensors.\n",
    "\n",
    "After running the method you can see the collapsed input tensors - `4*8 = 32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information about logits:\n",
      "Shape:\n",
      "torch.Size([32, 65])\n",
      "Logits (predictions for first character of first batch):\n",
      "tensor([ 0.3963, -1.4631, -1.0990, -1.7104, -1.2570, -0.2641, -0.3085, -0.5644,\n",
      "        -0.0534,  0.7400,  0.0912, -0.0041, -0.3235,  0.9601,  0.2023,  0.0994,\n",
      "        -0.6136, -2.0696,  0.4888, -0.7050,  0.7657, -1.0252, -0.6200, -0.8280,\n",
      "        -1.2047, -2.5844,  1.9835,  2.4489, -1.2784,  0.0163,  1.0204,  0.6234,\n",
      "        -0.4944, -0.5679,  0.7387,  0.2977,  1.5133,  0.0898,  0.3490, -1.4351,\n",
      "         1.2178, -0.7338,  0.2396, -0.0415,  0.3067,  2.1749, -0.0563,  1.0076,\n",
      "        -1.5035,  1.4801, -1.3473,  1.2003,  0.3616,  0.4924, -2.3997, -1.3982,\n",
      "         1.1088,  0.0864, -0.2992,  0.5236, -0.8487, -0.7711, -0.0528, -0.8631,\n",
      "        -0.4571], grad_fn=<SelectBackward0>)\n",
      "Index of predicted character:\n",
      "27\n",
      "Predicted character: O\n",
      "Overall loss:\n",
      "4.826590538024902\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        B, T, C = logits.shape \n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "m = BigramLanguageModel(vocab_size).to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(\"Information about logits:\")\n",
    "print(f\"Shape:\\n{logits.shape}\")\n",
    "print(f\"Logits (predictions for first character of first batch):\\n{logits[0]}\")\n",
    "char_idx = int(torch.argmax(logits[0]))\n",
    "print(f\"Index of predicted character:\\n{char_idx}\")\n",
    "print(f\"Predicted character: {decode([char_idx])}\")\n",
    "print(f\"Overall loss:\\n{loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write the `generate()` function that takes an input prompt (`idx`) and uses it as the starting point to predict the next character in the model. Now this model hasn't been trained at all, so there is no reason why it should produce anything other than gibberish which is what we see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.4065, grad_fn=<NllLossBackward0>)\n",
      "The context is tensor([[0]])\n",
      "The predictions are tensor([[ 0, 25, 59, 44, 30,  9,  2, 63, 15,  1, 26, 20, 23, 19, 37, 12, 15,  9,\n",
      "         43, 16, 17, 19, 25, 54,  7, 34, 15, 41, 38, 26,  9, 39, 53, 30, 26,  4,\n",
      "         54, 53, 61, 44, 36, 51, 14,  8, 17, 27, 36,  8, 39, 18, 50,  9, 57, 47,\n",
      "         58, 19, 48,  4, 35, 41, 19, 54, 53, 17, 44, 30, 60, 20, 24, 40, 30, 12,\n",
      "          0, 22, 62, 20, 53, 44, 30, 36,  6, 25, 41, 41, 13, 35, 14, 32, 18,  8,\n",
      "         13, 24, 18,  3, 51, 29, 11,  2, 19, 37, 10]])\n",
      "Decoded: \n",
      "MufR3!yC NHKGY?C3eDEGMp-VCcZN3aoRN&powfXmB.EOX.aFl3sitGj&WcGpoEfRvHLbR?\n",
      "JxHofRX,MccAWBTF.ALF$mQ;!GY:\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, debug=False):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        self.debug = debug\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape \n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def pprint(self, s, printed):\n",
    "        if self.debug and printed:\n",
    "            print(s)\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        printed = False\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the logits for the prediction - this is a vector where each\n",
    "            # input token has a score\n",
    "            logits, _ = self(idx)\n",
    "            self.pprint(f\"idx: {idx}\", printed)\n",
    "            self.pprint(f\"logits shape: {logits.shape}\", printed)\n",
    "            self.pprint(f\"logits: {logits}\", printed)\n",
    "            # Focus only on the last token - we are not using history to make\n",
    "            # predictions!\n",
    "            logits = logits[:, -1, :]\n",
    "            self.pprint(f\"logits shape: {logits.shape}\", printed)\n",
    "            self.pprint(f\"logits: {logits}\", printed)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            self.pprint(f\"probs shape: {probs.shape}\", printed)\n",
    "            self.pprint(f\"probs: {probs}\", printed)\n",
    "            # What algorithm do we use to sample from the distribution?\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            self.pprint(f\"Predicted next character: {idx_next}\", printed)\n",
    "            # Append to the resulting tensor. Note that we start with the \n",
    "            # original tensor, and then append the new token\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "            printed = True\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size).to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "predictions = m.generate(idx, max_new_tokens=100)\n",
    "print(f\"The context is {idx}\")\n",
    "print(f\"The predictions are {predictions}\")\n",
    "print(f\"Decoded: {decode(predictions[0].tolist())}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model. We'll begin by creating an optimizer object and then passing it into a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss = 2.540138006210327\n",
      "Training time: 6.33 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "m = BigramLanguageModel(vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "iterations = 8192\n",
    "\n",
    "start = time.perf_counter()\n",
    "for steps in range(iterations):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(f\"Final Loss = {loss.item()}\")\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"Training time: {elapsed:.2f} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the model. Note that this model is only using the last character in the sequence (and not the characters that came before it) to make the next prediction. So, not optimial, but if we squint really hard, we can see a bit of structure to what otherwise looks kind of like noise below:\n",
    "\n",
    "It seems like there is a real issue with inference time on GPU (probably because the `generate()` function doesn't do any clever batching at all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "grapeshesanche t heaswin;\n",
      "\n",
      "ATh; ame thirlameungavenr sll hieror:\n",
      "Tieasirurem, sn ks bers s nt aw,\n",
      "TE ong;\n",
      "Awaimfads seinsasthirefr as ct I neay wintheeeshe whas e!\n",
      "G t llerdedondd ay, heet m terefouna\n",
      "Inference time: 0.25 seconds\n"
     ]
    }
   ],
   "source": [
    "tokens_to_generate = 200\n",
    "start = time.perf_counter()\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long).to(device), max_new_tokens=tokens_to_generate)[0].tolist()))\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"Inference time: {elapsed:.2f} seconds or {tokens_to_generate/elapsed*1000:.2f} ms per token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
